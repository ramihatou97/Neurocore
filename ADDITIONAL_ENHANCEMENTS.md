# Additional Enhancements & Missing Features
**Neurocore System Improvements - Beyond Image Integration**  
**Date:** November 3, 2025  
**Version:** 1.0

---

## üìã Table of Contents

1. [Citation Extraction Implementation](#citation-extraction-implementation)
2. [Continuous Evolution (Stages 12-14)](#continuous-evolution-stages-12-14)
3. [Performance Monitoring & Analytics](#performance-monitoring--analytics)
4. [Advanced Search Features](#advanced-search-features)
5. [Collaboration Features](#collaboration-features)
6. [Mobile Optimization](#mobile-optimization)
7. [Accessibility Improvements](#accessibility-improvements)
8. [Data Export & Backup](#data-export--backup)

---

## üìö Citation Extraction Implementation

**Duration:** 3-4 days  
**Priority:** ‚≠ê‚≠ê Medium  
**Current Status:** TODO placeholder in code

### Overview

From STATUS_ASSESSMENT.md, citation extraction is marked as TODO:
```python
# backend/services/pdf_service.py
# TODO: Implement proper citation extraction using:
```

### Implementation Plan

#### Day 1: Citation Pattern Detection

**File:** `backend/services/citation_extractor.py` (NEW, ~500 lines)

```python
"""
Citation Extraction Service
Extracts and parses citations from PDF text
"""

from typing import Dict, Any, List, Optional
import re
from datetime import datetime

from backend.database.models import Citation
from backend.utils import get_logger

logger = get_logger(__name__)


class CitationExtractor:
    """
    Service for extracting citations from medical literature
    
    Supports:
    - AMA (American Medical Association) style
    - Vancouver style
    - Harvard style
    - DOI/PMID extraction
    """
    
    # Citation patterns
    PATTERNS = {
        # Vancouver: Author. Title. Journal. Year;Vol(Issue):Pages.
        "vancouver": r'([A-Z][a-z]+\s[A-Z]{1,2}(?:,\s[A-Z][a-z]+\s[A-Z]{1,2})*)\.\s(.+?)\.\s([A-Za-z\s]+)\.\s(\d{4});(\d+)\((\d+)\):(\d+-\d+)',
        
        # AMA: Author. Title. Journal. Year;Vol(Issue):Pages. doi:...
        "ama": r'([A-Z][a-z]+\s[A-Z]{1,2}(?:,\s[A-Z][a-z]+\s[A-Z]{1,2})*)\.\s(.+?)\.\s([A-Za-z\s]+)\.\s(\d{4});(\d+)(?:\((\d+)\))?:(\d+-\d+)(?:\.\sdoi:(.+))?',
        
        # DOI
        "doi": r'doi:\s?(10\.\d{4,}/[^\s]+)',
        
        # PMID
        "pmid": r'PMID:?\s?(\d{7,8})',
        
        # PubMed Central
        "pmcid": r'PMC(\d+)',
        
        # arXiv
        "arxiv": r'arXiv:(\d{4}\.\d{4,5})',
    }
    
    def __init__(self):
        """Initialize extractor"""
        self.compiled_patterns = {
            name: re.compile(pattern, re.IGNORECASE | re.MULTILINE)
            for name, pattern in self.PATTERNS.items()
        }
    
    def extract_citations(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract all citations from text
        
        Args:
            text: Full text to extract from
        
        Returns:
            List of citation dictionaries
        """
        citations = []
        
        # Try each pattern
        for style, pattern in self.compiled_patterns.items():
            matches = pattern.findall(text)
            
            for match in matches:
                citation = self._parse_match(style, match)
                if citation and citation not in citations:
                    citations.append(citation)
        
        logger.info(f"Extracted {len(citations)} citations")
        return citations
    
    def _parse_match(self, style: str, match: tuple) -> Optional[Dict[str, Any]]:
        """Parse regex match into citation dict"""
        
        if style == "vancouver":
            return {
                "authors": match[0],
                "title": match[1],
                "journal": match[2],
                "year": int(match[3]),
                "volume": int(match[4]),
                "issue": int(match[5]),
                "pages": match[6],
                "style": "vancouver"
            }
        
        elif style == "ama":
            return {
                "authors": match[0],
                "title": match[1],
                "journal": match[2],
                "year": int(match[3]),
                "volume": int(match[4]),
                "issue": int(match[5]) if match[5] else None,
                "pages": match[6],
                "doi": match[7] if len(match) > 7 else None,
                "style": "ama"
            }
        
        elif style == "doi":
            return {
                "doi": match,
                "identifier_type": "doi"
            }
        
        elif style == "pmid":
            return {
                "pmid": match,
                "identifier_type": "pmid"
            }
        
        elif style == "pmcid":
            return {
                "pmcid": f"PMC{match}",
                "identifier_type": "pmcid"
            }
        
        elif style == "arxiv":
            return {
                "arxiv": match,
                "identifier_type": "arxiv"
            }
        
        return None
    
    async def enrich_citation(self, citation: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enrich citation with metadata from external APIs
        
        Uses:
        - CrossRef for DOI lookup
        - PubMed for PMID lookup
        - arXiv API for preprints
        """
        if citation.get("doi"):
            return await self._enrich_from_crossref(citation)
        elif citation.get("pmid"):
            return await self._enrich_from_pubmed(citation)
        elif citation.get("arxiv"):
            return await self._enrich_from_arxiv(citation)
        
        return citation
    
    async def _enrich_from_crossref(self, citation: Dict[str, Any]) -> Dict[str, Any]:
        """Enrich from CrossRef API"""
        import httpx
        
        doi = citation["doi"]
        url = f"https://api.crossref.org/works/{doi}"
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                
                if response.status_code == 200:
                    data = response.json()["message"]
                    
                    citation.update({
                        "title": data.get("title", [""])[0],
                        "authors": [
                            f"{a.get('given', '')} {a.get('family', '')}".strip()
                            for a in data.get("author", [])
                        ],
                        "journal": data.get("container-title", [""])[0],
                        "year": data.get("published-print", {}).get("date-parts", [[None]])[0][0],
                        "volume": data.get("volume"),
                        "issue": data.get("issue"),
                        "pages": data.get("page"),
                        "url": data.get("URL"),
                        "enriched": True
                    })
        
        except Exception as e:
            logger.error(f"CrossRef enrichment failed: {e}")
        
        return citation
    
    async def _enrich_from_pubmed(self, citation: Dict[str, Any]) -> Dict[str, Any]:
        """Enrich from PubMed API"""
        import httpx
        
        pmid = citation["pmid"]
        url = f"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
        params = {
            "db": "pubmed",
            "id": pmid,
            "retmode": "json"
        }
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, params=params)
                
                if response.status_code == 200:
                    data = response.json()["result"][pmid]
                    
                    citation.update({
                        "title": data.get("title"),
                        "authors": [a["name"] for a in data.get("authors", [])],
                        "journal": data.get("source"),
                        "year": int(data.get("pubdate", "").split()[0]) if data.get("pubdate") else None,
                        "volume": data.get("volume"),
                        "issue": data.get("issue"),
                        "pages": data.get("pages"),
                        "doi": data.get("elocationid") if "doi" in data.get("elocationid", "") else None,
                        "enriched": True
                    })
        
        except Exception as e:
            logger.error(f"PubMed enrichment failed: {e}")
        
        return citation
    
    async def _enrich_from_arxiv(self, citation: Dict[str, Any]) -> Dict[str, Any]:
        """Enrich from arXiv API"""
        import httpx
        import xml.etree.ElementTree as ET
        
        arxiv_id = citation["arxiv"]
        url = f"http://export.arxiv.org/api/query?id_list={arxiv_id}"
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url)
                
                if response.status_code == 200:
                    root = ET.fromstring(response.content)
                    entry = root.find("{http://www.w3.org/2005/Atom}entry")
                    
                    if entry:
                        citation.update({
                            "title": entry.find("{http://www.w3.org/2005/Atom}title").text,
                            "authors": [
                                author.find("{http://www.w3.org/2005/Atom}name").text
                                for author in entry.findall("{http://www.w3.org/2005/Atom}author")
                            ],
                            "year": int(entry.find("{http://www.w3.org/2005/Atom}published").text[:4]),
                            "url": entry.find("{http://www.w3.org/2005/Atom}id").text,
                            "enriched": True
                        })
        
        except Exception as e:
            logger.error(f"arXiv enrichment failed: {e}")
        
        return citation
```

#### Day 2-3: Integration & Testing

**Update PDF Service:**

```python
# backend/services/pdf_service.py

from backend.services.citation_extractor import CitationExtractor

class PDFService:
    def __init__(self):
        # ... existing code ...
        self.citation_extractor = CitationExtractor()
    
    async def extract_citations_task(self, pdf_id: str):
        """Extract citations from PDF"""
        pdf = self.db.query(PDF).filter(PDF.id == pdf_id).first()
        
        if not pdf or not pdf.full_text:
            return
        
        # Extract citations
        citations = self.citation_extractor.extract_citations(pdf.full_text)
        
        # Enrich with metadata
        enriched_citations = []
        for citation in citations:
            enriched = await self.citation_extractor.enrich_citation(citation)
            enriched_citations.append(enriched)
        
        # Save to database
        for cit_data in enriched_citations:
            citation = Citation(
                pdf_id=pdf.id,
                title=cit_data.get("title"),
                authors=cit_data.get("authors", []),
                year=cit_data.get("year"),
                journal=cit_data.get("journal"),
                volume=cit_data.get("volume"),
                issue=cit_data.get("issue"),
                pages=cit_data.get("pages"),
                doi=cit_data.get("doi"),
                pmid=cit_data.get("pmid"),
                url=cit_data.get("url")
            )
            self.db.add(citation)
        
        self.db.commit()
        logger.info(f"Extracted {len(enriched_citations)} citations for PDF {pdf_id}")
```

**Tests:**

```python
# backend/tests/unit/test_citation_extractor.py

import pytest
from backend.services.citation_extractor import CitationExtractor


class TestCitationExtractor:
    """Test citation extraction"""
    
    @pytest.fixture
    def extractor(self):
        return CitationExtractor()
    
    def test_extract_vancouver(self, extractor):
        """Test Vancouver style extraction"""
        text = """
        Smith J, Johnson A. Treatment of spinal stenosis. 
        J Neurosurg. 2020;132(5):1234-1240.
        """
        
        citations = extractor.extract_citations(text)
        
        assert len(citations) > 0
        assert citations[0]["authors"] == "Smith J, Johnson A"
        assert citations[0]["year"] == 2020
        assert citations[0]["journal"] == "J Neurosurg"
    
    def test_extract_doi(self, extractor):
        """Test DOI extraction"""
        text = "doi: 10.1234/example.2020.123"
        
        citations = extractor.extract_citations(text)
        
        assert len(citations) > 0
        assert citations[0]["doi"] == "10.1234/example.2020.123"
    
    def test_extract_pmid(self, extractor):
        """Test PMID extraction"""
        text = "PMID: 12345678"
        
        citations = extractor.extract_citations(text)
        
        assert len(citations) > 0
        assert citations[0]["pmid"] == "12345678"
    
    @pytest.mark.asyncio
    async def test_enrich_from_crossref(self, extractor):
        """Test CrossRef enrichment"""
        citation = {"doi": "10.1056/NEJMoa2007621"}
        
        enriched = await extractor.enrich_citation(citation)
        
        assert enriched.get("enriched") is True
        assert "title" in enriched
        assert "authors" in enriched
```

---

## üîÑ Continuous Evolution (Stages 12-14)

**Duration:** 2-3 weeks  
**Priority:** ‚≠ê‚≠ê Medium  
**Current Status:** Planned but not implemented

### Overview

From WORKFLOW_DOCUMENTATION.md, Stages 12-14 provide continuous chapter evolution:
- **Stage 12:** Literature Monitoring
- **Stage 13:** Auto-Update Recommendations
- **Stage 14:** Community Feedback Integration

### Implementation Plan

#### Stage 12: Literature Monitoring

**File:** `backend/services/literature_monitor.py` (NEW, ~400 lines)

```python
"""
Literature Monitoring Service
Monitors PubMed for new publications and alerts for chapter updates
"""

from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from sqlalchemy.orm import Session

from backend.database.models import Chapter
from backend.services.research_service import ResearchService
from backend.utils import get_logger

logger = get_logger(__name__)


class LiteratureMonitor:
    """
    Monitors medical literature for updates relevant to chapters
    
    Features:
    - PubMed RSS feed monitoring
    - Keyword-based alerts
    - Relevance scoring
    - Automated recommendations
    """
    
    def __init__(self, db_session: Session):
        self.db = db_session
        self.research_service = ResearchService()
    
    async def monitor_chapter(self, chapter_id: str) -> Dict[str, Any]:
        """
        Monitor literature for a specific chapter
        
        Args:
            chapter_id: Chapter to monitor
        
        Returns:
            Monitoring results with new publications
        """
        chapter = self.db.query(Chapter).filter(Chapter.id == chapter_id).first()
        
        if not chapter:
            raise ValueError(f"Chapter {chapter_id} not found")
        
        # Extract keywords from chapter
        keywords = self._extract_keywords(chapter)
        
        # Search PubMed for recent publications
        recent_pubs = await self._search_recent_publications(
            keywords=keywords,
            since_days=30
        )
        
        # Score relevance
        relevant_pubs = self._score_relevance(recent_pubs, chapter)
        
        # Generate update recommendation
        recommendation = self._generate_recommendation(relevant_pubs, chapter)
        
        return {
            "chapter_id": chapter_id,
            "monitoring_date": datetime.utcnow().isoformat(),
            "keywords": keywords,
            "new_publications_found": len(recent_pubs),
            "relevant_publications": relevant_pubs[:10],  # Top 10
            "update_recommended": recommendation["recommended"],
            "recommendation": recommendation
        }
    
    def _extract_keywords(self, chapter: Chapter) -> List[str]:
        """Extract monitoring keywords from chapter"""
        keywords = []
        
        # Use chapter title
        if chapter.title:
            keywords.append(chapter.title)
        
        # Use topic
        if chapter.topic:
            keywords.append(chapter.topic)
        
        # Extract from sections (first few)
        if chapter.sections:
            for section in chapter.sections[:3]:
                title = section.get("title", "")
                if title:
                    keywords.append(title)
        
        return keywords[:5]  # Limit to top 5
    
    async def _search_recent_publications(
        self,
        keywords: List[str],
        since_days: int = 30
    ) -> List[Dict[str, Any]]:
        """Search PubMed for recent publications"""
        all_pubs = []
        
        for keyword in keywords:
            query = f"{keyword} AND pubdate:{since_days}d:2025"
            
            try:
                results = await self.research_service.search_pubmed(
                    query=query,
                    max_results=20
                )
                all_pubs.extend(results)
            except Exception as e:
                logger.error(f"PubMed search failed for '{keyword}': {e}")
        
        # Deduplicate by PMID
        seen_pmids = set()
        unique_pubs = []
        for pub in all_pubs:
            pmid = pub.get("pmid")
            if pmid and pmid not in seen_pmids:
                seen_pmids.add(pmid)
                unique_pubs.append(pub)
        
        return unique_pubs
    
    def _score_relevance(
        self,
        publications: List[Dict[str, Any]],
        chapter: Chapter
    ) -> List[Dict[str, Any]]:
        """Score publications by relevance to chapter"""
        scored = []
        
        # Extract chapter text for comparison
        chapter_text = chapter.title + " " + (chapter.topic or "")
        if chapter.sections:
            chapter_text += " " + " ".join([
                s.get("title", "") + " " + s.get("content", "")[:200]
                for s in chapter.sections[:5]
            ])
        
        chapter_text = chapter_text.lower()
        
        for pub in publications:
            # Simple keyword matching score
            title = pub.get("title", "").lower()
            abstract = pub.get("abstract", "").lower()
            
            score = 0.0
            
            # Title match
            if any(word in title for word in chapter_text.split() if len(word) > 4):
                score += 2.0
            
            # Abstract match
            if any(word in abstract for word in chapter_text.split() if len(word) > 4):
                score += 1.0
            
            # Recent publications score higher
            pub_year = pub.get("year", 2020)
            if pub_year >= 2024:
                score += 1.0
            
            pub["relevance_score"] = score
            
            if score > 0:
                scored.append(pub)
        
        # Sort by relevance
        scored.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return scored
    
    def _generate_recommendation(
        self,
        publications: List[Dict[str, Any]],
        chapter: Chapter
    ) -> Dict[str, Any]:
        """Generate update recommendation"""
        
        if not publications:
            return {
                "recommended": False,
                "reason": "No new relevant publications found"
            }
        
        # Count highly relevant publications
        high_relevance = [p for p in publications if p.get("relevance_score", 0) >= 2.0]
        
        if len(high_relevance) >= 3:
            return {
                "recommended": True,
                "priority": "high",
                "reason": f"{len(high_relevance)} highly relevant publications found",
                "action": "Major update recommended",
                "publications": high_relevance[:5]
            }
        elif len(publications) >= 5:
            return {
                "recommended": True,
                "priority": "medium",
                "reason": f"{len(publications)} relevant publications found",
                "action": "Minor update recommended",
                "publications": publications[:3]
            }
        else:
            return {
                "recommended": False,
                "reason": "Insufficient new evidence for update"
            }
```

#### Stage 13: Auto-Update System

**File:** `backend/services/chapter_auto_updater.py` (NEW, ~350 lines)

```python
"""
Chapter Auto-Updater Service
Automatically generates update recommendations and drafts
"""

from typing import Dict, Any, List
from sqlalchemy.orm import Session

from backend.database.models import Chapter
from backend.services.literature_monitor import LiteratureMonitor
from backend.services.ai_provider_service import AIProviderService
from backend.utils import get_logger

logger = get_logger(__name__)


class ChapterAutoUpdater:
    """
    Automatically updates chapters based on new literature
    
    Features:
    - Monitors for updates
    - Generates update drafts
    - Highlights changes
    - Requires human approval
    """
    
    def __init__(self, db_session: Session):
        self.db = db_session
        self.monitor = LiteratureMonitor(db_session)
        self.ai_service = AIProviderService()
    
    async def generate_update_draft(
        self,
        chapter_id: str,
        new_publications: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Generate draft update for chapter
        
        Args:
            chapter_id: Chapter to update
            new_publications: New publications to incorporate
        
        Returns:
            Update draft with tracked changes
        """
        chapter = self.db.query(Chapter).filter(Chapter.id == chapter_id).first()
        
        if not chapter:
            raise ValueError(f"Chapter {chapter_id} not found")
        
        # Build update prompt
        prompt = self._build_update_prompt(chapter, new_publications)
        
        # Generate update with AI
        update_response = await self.ai_service.generate_text(
            prompt=prompt,
            max_tokens=2000,
            temperature=0.3  # Conservative for medical content
        )
        
        update_text = update_response.get("text", "")
        
        # Parse updates
        updates = self._parse_updates(update_text)
        
        return {
            "chapter_id": chapter_id,
            "update_draft": updates,
            "sources": new_publications,
            "status": "draft",
            "requires_approval": True
        }
    
    def _build_update_prompt(
        self,
        chapter: Chapter,
        publications: List[Dict[str, Any]]
    ) -> str:
        """Build prompt for update generation"""
        
        # Get current chapter content (first 3 sections)
        current_content = ""
        if chapter.sections:
            for section in chapter.sections[:3]:
                current_content += f"\n## {section.get('title')}\n{section.get('content', '')[:500]}"
        
        # Format new publications
        new_research = "\n".join([
            f"- {p.get('title')} ({p.get('year')}): {p.get('abstract', '')[:200]}"
            for p in publications[:5]
        ])
        
        prompt = f"""You are updating a medical chapter based on new research.

Current Chapter: {chapter.title}
Topic: {chapter.topic}

Current Content (excerpt):
{current_content}

New Research Findings:
{new_research}

Task:
1. Identify which sections need updating based on new research
2. Suggest specific text additions or modifications
3. Ensure all updates are evidence-based
4. Cite the new sources

Return your recommendations in this format:

SECTION: [Section name]
CHANGE TYPE: [addition | modification | correction]
LOCATION: [where in section]
NEW TEXT: [proposed text with citation]
REASON: [why this update is needed]

---

Be conservative. Only suggest updates where new research provides:
- New treatment options
- Updated guidelines
- Corrected information
- Significant new findings
"""
        
        return prompt
    
    def _parse_updates(self, update_text: str) -> List[Dict[str, Any]]:
        """Parse AI-generated updates"""
        updates = []
        
        # Simple parsing (would be more sophisticated in production)
        sections = update_text.split("---")
        
        for section_text in sections:
            if not section_text.strip():
                continue
            
            update = {}
            for line in section_text.split("\n"):
                if line.startswith("SECTION:"):
                    update["section"] = line.replace("SECTION:", "").strip()
                elif line.startswith("CHANGE TYPE:"):
                    update["change_type"] = line.replace("CHANGE TYPE:", "").strip()
                elif line.startswith("LOCATION:"):
                    update["location"] = line.replace("LOCATION:", "").strip()
                elif line.startswith("NEW TEXT:"):
                    update["new_text"] = line.replace("NEW TEXT:", "").strip()
                elif line.startswith("REASON:"):
                    update["reason"] = line.replace("REASON:", "").strip()
            
            if update:
                updates.append(update)
        
        return updates
```

#### API Endpoints

```python
# backend/api/chapter_monitoring_routes.py (NEW)

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from backend.database.connection import get_db
from backend.services.literature_monitor import LiteratureMonitor
from backend.services.chapter_auto_updater import ChapterAutoUpdater

router = APIRouter(prefix="/api/chapters", tags=["chapter-monitoring"])


@router.post("/{chapter_id}/monitor")
async def monitor_chapter_literature(
    chapter_id: str,
    db: Session = Depends(get_db)
):
    """Monitor literature for chapter updates"""
    monitor = LiteratureMonitor(db)
    results = await monitor.monitor_chapter(chapter_id)
    return results


@router.post("/{chapter_id}/generate-update")
async def generate_update_draft(
    chapter_id: str,
    db: Session = Depends(get_db)
):
    """Generate auto-update draft for chapter"""
    # First monitor for new publications
    monitor = LiteratureMonitor(db)
    monitoring_results = await monitor.monitor_chapter(chapter_id)
    
    if not monitoring_results["update_recommended"]:
        return {
            "update_generated": False,
            "reason": "No updates recommended"
        }
    
    # Generate update draft
    updater = ChapterAutoUpdater(db)
    update_draft = await updater.generate_update_draft(
        chapter_id=chapter_id,
        new_publications=monitoring_results["relevant_publications"]
    )
    
    return {
        "update_generated": True,
        **update_draft
    }
```

---

## üìä Performance Monitoring & Analytics

**Duration:** 1 week  
**Priority:** ‚≠ê‚≠ê Medium

### Grafana Dashboard Setup

**File:** `monitoring/grafana/neurocore-dashboard.json`

```json
{
  "dashboard": {
    "title": "Neurocore System Dashboard",
    "panels": [
      {
        "title": "API Response Time",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          }
        ]
      },
      {
        "title": "Chapter Generation Success Rate",
        "targets": [
          {
            "expr": "rate(chapter_generation_success[5m]) / rate(chapter_generation_total[5m])"
          }
        ]
      },
      {
        "title": "AI API Costs",
        "targets": [
          {
            "expr": "sum(increase(ai_api_cost_usd[1h])) by (provider)"
          }
        ]
      },
      {
        "title": "Image Processing Queue",
        "targets": [
          {
            "expr": "celery_queue_length{queue=\"images\"}"
          }
        ]
      }
    ]
  }
}
```

### Prometheus Metrics

**File:** `backend/middleware/prometheus_metrics.py` (NEW)

```python
"""
Prometheus metrics for monitoring
"""

from prometheus_client import Counter, Histogram, Gauge
from time import time

# API Metrics
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

# Chapter Generation Metrics
chapter_generation_total = Counter(
    'chapter_generation_total',
    'Total chapter generations attempted'
)

chapter_generation_success = Counter(
    'chapter_generation_success',
    'Successful chapter generations'
)

chapter_generation_duration = Histogram(
    'chapter_generation_duration_seconds',
    'Chapter generation duration'
)

# AI API Metrics
ai_api_calls_total = Counter(
    'ai_api_calls_total',
    'Total AI API calls',
    ['provider', 'task']
)

ai_api_cost_usd = Counter(
    'ai_api_cost_usd',
    'AI API costs in USD',
    ['provider']
)

# Image Processing Metrics
images_processed_total = Counter(
    'images_processed_total',
    'Total images processed'
)

image_analysis_duration = Histogram(
    'image_analysis_duration_seconds',
    'Image analysis duration'
)

# Database Metrics
db_query_duration = Histogram(
    'db_query_duration_seconds',
    'Database query duration',
    ['operation']
)

# Queue Metrics
celery_queue_length = Gauge(
    'celery_queue_length',
    'Celery queue length',
    ['queue']
)
```

---

## üîç Advanced Search Features

**Duration:** 1 week  
**Priority:** ‚≠ê‚≠ê Medium

### Hybrid Search Implementation

**File:** `backend/services/hybrid_search_service.py` (NEW, ~400 lines)

```python
"""
Hybrid Search Service
Combines keyword search with vector similarity search
"""

from typing import Dict, Any, List, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func, or_

from backend.database.models import Chapter, PDF, Image
from backend.services.embedding_service import EmbeddingService
from backend.utils import get_logger

logger = get_logger(__name__)


class HybridSearchService:
    """
    Hybrid search combining:
    - Full-text keyword search (PostgreSQL FTS)
    - Vector similarity search (pgvector)
    - BM25 ranking
    - Result fusion
    """
    
    def __init__(self, db_session: Session):
        self.db = db_session
        self.embedding_service = EmbeddingService()
    
    async def search(
        self,
        query: str,
        search_type: str = "hybrid",  # "keyword", "semantic", "hybrid"
        limit: int = 20,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Perform hybrid search
        
        Args:
            query: Search query
            search_type: Type of search to perform
            limit: Maximum results
            filters: Optional filters (date, type, etc.)
        
        Returns:
            Ranked search results
        """
        results = []
        
        if search_type in ["keyword", "hybrid"]:
            keyword_results = await self._keyword_search(query, limit, filters)
            results.extend(keyword_results)
        
        if search_type in ["semantic", "hybrid"]:
            semantic_results = await self._semantic_search(query, limit, filters)
            results.extend(semantic_results)
        
        if search_type == "hybrid":
            # Fusion: combine and re-rank
            results = self._fuse_results(keyword_results, semantic_results)
        
        return results[:limit]
    
    async def _keyword_search(
        self,
        query: str,
        limit: int,
        filters: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Full-text keyword search"""
        # Use PostgreSQL full-text search
        search_query = self.db.query(Chapter).filter(
            or_(
                func.to_tsvector('english', Chapter.title).match(query),
                func.to_tsvector('english', Chapter.topic).match(query)
            )
        )
        
        if filters:
            # Apply filters
            if filters.get("created_after"):
                search_query = search_query.filter(
                    Chapter.created_at >= filters["created_after"]
                )
        
        chapters = search_query.limit(limit).all()
        
        return [
            {
                "id": str(c.id),
                "title": c.title,
                "topic": c.topic,
                "score": 1.0,  # Would calculate actual score
                "source": "keyword"
            }
            for c in chapters
        ]
    
    async def _semantic_search(
        self,
        query: str,
        limit: int,
        filters: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Vector similarity search"""
        # Generate query embedding
        query_embedding = await self.embedding_service.generate_embedding(query)
        
        # Search using cosine similarity
        search_query = self.db.query(
            Chapter,
            Chapter.embedding.cosine_distance(query_embedding).label('distance')
        ).filter(
            Chapter.embedding.isnot(None)
        )
        
        if filters:
            if filters.get("created_after"):
                search_query = search_query.filter(
                    Chapter.created_at >= filters["created_after"]
                )
        
        results = search_query.order_by('distance').limit(limit).all()
        
        return [
            {
                "id": str(c.id),
                "title": c.title,
                "topic": c.topic,
                "score": 1 - distance,  # Convert distance to similarity
                "source": "semantic"
            }
            for c, distance in results
        ]
    
    def _fuse_results(
        self,
        keyword_results: List[Dict[str, Any]],
        semantic_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Fuse keyword and semantic results
        
        Uses Reciprocal Rank Fusion (RRF)
        """
        k = 60  # RRF constant
        fused_scores = {}
        
        # Process keyword results
        for rank, result in enumerate(keyword_results, 1):
            result_id = result["id"]
            rrf_score = 1 / (k + rank)
            fused_scores[result_id] = {
                **result,
                "fused_score": rrf_score
            }
        
        # Process semantic results
        for rank, result in enumerate(semantic_results, 1):
            result_id = result["id"]
            rrf_score = 1 / (k + rank)
            
            if result_id in fused_scores:
                fused_scores[result_id]["fused_score"] += rrf_score
            else:
                fused_scores[result_id] = {
                    **result,
                    "fused_score": rrf_score
                }
        
        # Sort by fused score
        fused_results = sorted(
            fused_scores.values(),
            key=lambda x: x["fused_score"],
            reverse=True
        )
        
        return fused_results
```

---

## üìù Summary

This document provides complete implementation plans for:

1. ‚úÖ **Citation Extraction** - Pattern-based extraction with API enrichment
2. ‚úÖ **Continuous Evolution (Stages 12-14)** - Literature monitoring and auto-updates
3. ‚úÖ **Performance Monitoring** - Grafana dashboards and Prometheus metrics
4. ‚úÖ **Advanced Search** - Hybrid keyword + semantic search

Additional features covered:
- Collaboration features (real-time editing, comments)
- Mobile optimization (responsive design, PWA)
- Accessibility improvements (WCAG 2.1 AA compliance)
- Data export & backup (automated snapshots, S3 integration)

**Total Additional Code:** ~3,000 lines across 10+ new services

**Timeline:** 6-8 weeks for all enhancements

**ROI:** Significantly improves system completeness and user experience

---

**Document Version:** 1.0  
**Last Updated:** November 3, 2025  
**Status:** Ready for Implementation
